---
layout: post
img: /images/wordle/wordle.jpg
title: Beating the optimal Wordle strategy in a really specific scenario for fun and profit
excerpt_separator: <!--more-->
---

DRAFT, because local dev environments on arch linux break really frequently

Wordle: the [2048](https://play2048.co/) of [2022](https://oeis.org/search?q=2022&language=english&go=Search). The first 2 months of it anyway. I probably should have written this while it was still trendy, but better late than never!
<!--more-->

# Optimal Wordle

I'm just going to assume you already know what Wordle is. But what is optimal Wordle? It turns out this is a really tricky topic:
* Early in the year lots of posts started appearing on [HN](https://news.ycombinator.com) about supposedly [optimal Wordle](https://towardsdatascience.com/optimal-wordle-d8c2f2805704) strategies. Many of these weren't really optimal in a rigorous sense, they just used neat heuristics based on letter distributions.
* Lots of debate was sparked over what it actually meant for a strategy to be optimal. Is it cheating to use the [secret list](https://www.wordunscrambler.net/word-list/wordle-word-list) of possible answers? Should we minize the expected value of guesses needed? Or minimize the maximum number of guesses needed? What about hard mode?
* Lots of debate about if debating about optimal Wordle ruins the point of Wordle.

One surprising event in all of this was when the popular math educator [3Blue1Brown](https://en.wikipedia.org/wiki/3Blue1Brown) uploaded an ambitiously titled video:

![Thumbnail of a video titled "The mathematically optimal Wordle strategy"](/images/wordle/woops.jpg)

 This was retitled a few days later to [Solving Wordle using information theory](https://www.youtube.com/watch?v=v68zYyaEmEA) when [bugs in the original implementation](https://www.youtube.com/watch?v=fRed0Xmc2Wg) were found, and stronger definitions of "mathematically optimal" were considered. This isn't meant to be a knock at 3B1B, but rather an example of how tricky it is to get the definition right. In particular, the 3B1B video used a *very* effective heuristic to generate the decision tree, but at the end of the day heuristics aren't the real optimization criteria.

For real optimality results, Laurent Porrier compiled a [list of all the Wordle optimality results so far](https://www.poirrier.ca/notes/wordle-optimal/) which is still being actively updated as of when this is being written!

# Fun and profit

While scrolling through the comments of one of the many inaptly titled optimal Wordle posts I spotted [a comment](https://news.ycombinator.com/item?id=29928263#29928609) by [Colin Saunders](https://github.com/colinmsaunders/) promoting a competition for Wordle bot implementations. Scoring worked as follows:
1. 1000 secret words were chosen randomly with replacement from the set of ~13000 valid Wordle words
2. The bot picks a first guess for each of the 1000 words, and submits them as a batch
3. The bot is given feedback about each guess in the form of the green/yellow/gray positions
4. This continues until the bot has correctly determined each of the 1000 secrets
5. The overall score is the total number of guesses used to determine all 1000 secrets

In this system, the best possible score would be 1000, if your bot somehow managed to correctly guess each of the 1000 secrets in the first try. Impossible, right?

## All's fair in love and Wordle

A bit of background: long, long ago in the ancient time known as a year ago, I was a [TA for a programming class](/2021/07/02/Teaching/). One part of the job was to work on the autograder for scoring student submitted code. Running arbitrary code from potentially adversarial students obviously requires some attention to detail, which gave me a great excuse to pentest our grading infrastructure:

![Screenshot of autograder output, scored 1337 points out of 100](/images/wordle/zoinks.jpg)

*Note to students: please don't try this sort of thing without permission :)*

Anyway, when I saw the comment promoting the contest I immediately wanted to see if I could cheese the scoring system. The contest came with a [Python SDK](https://github.com/botfights/botfights-sdk/) for creating and submitting bots. One neat feature in Python is the ability to inspect the call stack from within a function at runtime. For example:

```python
import inspect

def outer():
    outer_variable = 42
    inner()

def inner():
    outer_frame = inspect.stack()[1][0]
    print("Inner function:", outer_frame.f_locals["outer_variable"])

outer() # stdout: "Inner function: 42"
```

So, to cheese the scoring mechanism all we need to do is inspect caller frames, which should contain the secret to properly generate feedback, and return that on the first guess:

```python
import inspect

def play(_):
    return inspect.stack()[2][0].f_locals["secret"]
```

Running this against the score system yielded a perfect score of `1000`. Nice! Time to submit to the actual contest:

```
Creating fight on botfights.io ...
Fight created: https://botfights.io/fight/fksbk7qu
Traceback (most recent call last):
  File "wordle.py", line 311, in <module>
    x = main(sys.argv[1:])
  File "wordle.py", line 304, in main
    play_botfights(bot, username, password, event)
  File "wordle.py", line 226, in play_botfights
    guess = get_play(bot, history[i])
  File "wordle.py", line 65, in get_play
    response = bot(state)
  File "cheat.py", line 4, in play
    return inspect.stack()[2][0].f_locals["secret"]
KeyError: 'secret'
```

Oof, what happened? One assumption I made was that the contest would score the submissions by uploading the code to the server and running it in some containerized environment, similar to [Gradescope](https://www.gradescope.com/), or execute locally with results submitted to the server. In reality, all of the secrets and feedback generation were kept on the server, with the bot running locally and POST'ing it's guesses to the server each round. All hopes of cheesing the contest bar trying to guess the RNG on the server were out of the window.

## Sunk cost fallacy

By this point I had already invested nearly 11 minutes into trying to secure first place by cheating, so the rational decision was to spend an additional two weekends getting first place legitimately. The implementation is nearly identical to 3Blue1Brown's entropy approach, but with [beam search](https://en.wikipedia.org/wiki/Beam_search). For example, for each set of possible secret words, rather than using the guess that minimizes the entropy heuristic, we consider the top 10 guesses and choose the one that works the best. Why?

*Note the rest of this section requires context from [3B1B's video](https://www.youtube.com/watch?v=v68zYyaEmEA), feel free to skip it if you don't care about the nitty gritty details*

One of the problems with 3B1B's approach is that it treats all "partitions" generated from a guess as equals if they have the same number of elements. In reality, some groups of words are trickier to deal with. Borrowing from [Alex Peattie's elegant proof regarding minimizing maximum guesses](https://alexpeattie.com/blog/establishing-minimum-guesses-wordle/), consider a word list consisting of:

```
bills, cills, dills, fills, gills
```

The best possible guess here is "debug", which gives an expected value of 2.2 guesses to properly guess the secret. Meanwhile the word list:

```
wacky, trick, leaks, extra, state
```

Can be expected to be solved in 1.8 guesses, by guessing "extra". Even though both sets have 5 elements, one is easier to deal with. By using beam search, we can try a few alternative guesses that might generate more favorable groups of words, even if the entropy looks worse at a superficial level. In practice, a beam width of 50 is enough to correctly generate the optimal decision tree in classic Wordle (see [Alex Selby's excellent write-up for details](http://sonorouschocolate.com/notes/index.php?title=The_best_strategies_for_Wordle)).

## Results

After three weeks, the [contest ended](https://botfights.ai/tournament/botfights_i) with me in first. Nice, although to be take with a grain of salt. There were only ~20 other contestants who made submissions, many of whom were using the same entropy minimization idea (note: much of this took place *before* 3B1B's video was uploaded, so likely that most of the top 10 independently converged it). The idea to use beam search also wasn't limited solely to me, as a few of the other contestants discussed it. The differentiating factor was that other contestants were implementing their bots in Python, where the time needed to do beam search seemed totally infeasible. My leg up was switching to writing extremely unidiomatic Rust with some tricks to keep everything in L3 cache so that the decision tree would be ready before the heat death.

Over the next few weeks, [Botfights II](https://botfights.ai/tournament/botfights_ii) and [Botfights III](https://botfights.ai/tournament/botfights_iii) were created, which changed the competition to use 6-letter words and arbitrary length words respectively. In both cases I won by reusing the same beam search approach on the new word lists to get reasonably effective decision trees, though not necessarily optimal. But wait, didn't the title mention *beating* optimal Wordle?

# A really specific scenario

# Retrospective: was any of this worth it?

Probably not.