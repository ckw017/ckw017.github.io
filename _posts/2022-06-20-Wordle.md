---
layout: post
img: /images/wordle/wordle.jpg
title: Beating the optimal Wordle strategy in a really specific scenario
excerpt_separator: <!--more-->
---

This is a WIP on prod, because local dev environments on arch linux break really frequently :')

Wordle: the [2048](https://play2048.co/) of [2022](https://oeis.org/search?q=2022&language=english&go=Search). The first 2 months of it anyway. I probably should have written this while it was still trendy, but better late than never!
<!--more-->

# What is optimal Wordle?

I'm just going to assume you already know what Wordle is. But what is optimal Wordle? It turns out to be a controversial topic:
* Early in the year lots of posts started appearing on [Hacker News](https://news.ycombinator.com) about supposedly [optimal Wordle](https://towardsdatascience.com/optimal-wordle-d8c2f2805704) strategies. Many of these weren't really optimal, they just used reasonably effective heuristics based on letter distributions.
* Lots of debate was sparked over what it actually meant for a strategy to be optimal. Is it cheating to use the [secret list](https://www.wordunscrambler.net/word-list/wordle-word-list) of possible answers? Should we minize the expected value of guesses needed? Or minimize the maximum number of guesses needed? What about hard mode?
* Lots of debate about if debating about optimal Wordle is productive or ruins the point of Wordle.

One surprising event in all of this was when the popular math educator [3Blue1Brown](https://en.wikipedia.org/wiki/3Blue1Brown) uploaded an ambitiously titled video:

![Thumbnail of a video titled "The mathematically optimal Wordle strategy"](/images/wordle/woops.jpg)

 This was retitled a few days later to [Solving Wordle using information theory](https://www.youtube.com/watch?v=v68zYyaEmEA) when [bugs in the original implementation](https://www.youtube.com/watch?v=fRed0Xmc2Wg) were found, although even without the bugs the solution generated wasn't necessarily optimal. This isn't meant to be a knock at 3B1B, but rather an example of how tricky it is to get the definition right. In particular, the 3B1B video used a *very* effective heuristic to generate the decision tree, but at the end of the day heuristics aren't the real optimization criteria.

For real optimality results, Laurent Porrier compiled a [list of all the Wordle optimality results so far](https://www.poirrier.ca/notes/wordle-optimal/) which is still being actively updated as of when this is being written! In this post, optimal refers to any strategy that minimizes the expected number of guesses needed to solve a word uniformly sampled from the secret list.

# A competition

While scrolling through the comments of one of the ambitiously titled optimal Wordle posts I spotted [a comment](https://news.ycombinator.com/item?id=29928263#29928609) by [Colin Saunders](https://github.com/colinmsaunders/) promoting a competition for Wordle bot implementations. Scoring worked as follows:
1. 1000 secret words were chosen randomly with replacement from the set of 12972 valid Wordle words
2. The bot picks a first guess for each of the 1000 words, and submits them as a batch
3. The bot is given feedback about each guess in the form of the green/yellow/gray positions
4. This continues until the bot has correctly determined each of the 1000 secrets
5. The overall score is the total number of guesses used to determine all 1000 secrets

![](/images/wordle/diagram.jpg)

In this system, the best possible score would be 1000, if your bot somehow managed to correctly guess each of the 1000 secrets on the first turn. Impossible, right?

## All's fair in love and Wordle

Background: long, long ago in the forgotten era known as about a year ago, I was a [TA for a programming class](/2021/07/02/Teaching/). One part of the job was to work on the autograder for scoring student submitted code. Running arbitrary code from potentially adversarial students requires some attention to detail. In other words, I had a great excuse to practice pentesting on our grading infrastructure:

![Screenshot of autograder output, scored 1337 points out of 100](/images/wordle/zoinks.jpg)

*Note to students: please don't try this sort of thing without your instructors permission :)*

When I saw the comment promoting the contest I wanted to see if I could cheese the scoring system. The contest came with a [Python SDK](https://github.com/botfights/botfights-sdk/) for creating and submitting bots. One neat utility in Python lets you inspect the call stack at runtime. For example:

```python
import inspect

def outer():
    outer_variable = 42
    inner()

def inner():
    outer_frame = inspect.stack()[1][0]
    print("Inner function:", outer_frame.f_locals["outer_variable"])

outer() # stdout: "Inner function: 42"
```

So, to cheese the scoring mechanism all we need to do is inspect caller frames, which should contain the secret to properly generate feedback, and return that on the first guess:

```python
import inspect

def play(_):
    return inspect.stack()[2][0].f_locals["secret"]
```

Running this locally yielded a perfect score of `1000`. Nice! Time to submit to the actual contest:

```
Creating fight on botfights.io ...
Fight created: https://botfights.io/fight/fksbk7qu
Traceback (most recent call last):
  File "wordle.py", line 311, in <module>
    x = main(sys.argv[1:])
  File "wordle.py", line 304, in main
    play_botfights(bot, username, password, event)
  File "wordle.py", line 226, in play_botfights
    guess = get_play(bot, history[i])
  File "wordle.py", line 65, in get_play
    response = bot(state)
  File "cheat.py", line 4, in play
    return inspect.stack()[2][0].f_locals["secret"]
KeyError: 'secret'
```

Oof, what happened? One assumption I made was that the contest would score submissions by uploading code and running it in some containerized environment, similar to [Gradescope](https://www.gradescope.com/). In reality, all the secrets and feedback mechanism were kept on the server, with the bot running locally and sending guesses to the server each round. All hopes of cheesing the contest were out the window, bar trying to manipulate the server's RNG.

## Sunk cost fallacy

At this point I had invested almost eleven minutes of my life into trying to secure first place illegitimately, so the rational decision was to spend an additional two weekends trying to get first place legitimately. My implementation was nearly identical to 3Blue1Brown's entropy approach, but with [beam search](https://en.wikipedia.org/wiki/Beam_search). Given a set of possible secret words, rather than using the guess that minimizes the entropy heuristic, we consider the top 10 guesses and choose the one that works the best. Why?

## Why beam search?

*Note this section requires context from [3B1B's video](https://www.youtube.com/watch?v=v68zYyaEmEA), feel free to skip it if you don't care about the nitty gritty details*

One of the problems with 3B1B's approach is that it treats all "partitions" generated from a guess as equals if they have the same number of elements, and assumes that smaller partitions are always better than larger ones. In reality, some sets of words are trickier to deal with even if they have the same or fewer number of elements. Borrowing from [Alex Peattie's elegant proof regarding minimizing maximum guesses](https://alexpeattie.com/blog/establishing-minimum-guesses-wordle/), consider a word list consisting of:

```
bills, cills, dills, fills, gills
```

The best possible guess here is "debug", which gives an expected value of 2.2 guesses to properly guess the secret. Meanwhile the word list:

```
wacky, trick, leaks, extra, state
```

Can be expected to be solved in 1.8 guesses, by guessing "extra". Even though both sets have 5 elements, one is easier to deal with. By using beam search, we can try other viable guesses that might generate more favorable groups of words, even if the entropy looks worse at a superficial level. In practice, a beam width of 50 is enough to correctly generate the optimal decision tree in classic Wordle (see [Alex Selby's excellent write-up for details](http://sonorouschocolate.com/notes/index.php?title=The_best_strategies_for_Wordle)).

## Results

After three weeks, the [contest ended](https://botfights.ai/tournament/botfights_i) and I came out on top. Nice! Although to be take with a grain of salt. There were only ~20 other contestants who made submissions, many of whom were using the same entropy minimization idea. Much of this took place *before* 3B1B's video was uploaded, so its likely that most people independently converged on it. The idea to use beam search also wasn't limited to me either, as a few of the other contestants discussed it. The differentiating factor was that other contestants were working in Python, where the time needed to do beam search seemed totally infeasible. Early on I switched to Rust and did some tricks with caching (both L3 and application level) so that the decision tree would be ready before the heat death of the universe.

Over the next few weeks, [Botfights II](https://botfights.ai/tournament/botfights_ii) and [Botfights III](https://botfights.ai/tournament/botfights_iii) were created, which used 6-letter words and arbitrary length words as secrets respectively. In both cases I won by reusing the same beam search approach on the new word lists to get reasonably effective decision trees, though not necessarily optimal. But wait, didn't the title mention *beating* optimal Wordle?

# A really specific scenario

As you might guess, the next event was [Botfights IV](https://botfights.ai/tournament/botfights_iv). Previous iterations of the contest kept things interesting by increasing the number of potential hidden words. The twist in this contest was a bit different:
* The contest would return to the original list of 12972 five-letter words
* All of the secret words would be chosen from the 2315 words in the [secret list](https://www.wordunscrambler.net/word-list/wordle-word-list) that the original Wordle website uses (rather than sampled from every valid five-letter word)
* Instead of testing the submission against a random sample (with replacement) of 1000 words, each submission is tested against all 2315 of the possible secret words (sampled without replacement). In other words, each submission was tested against a permutation of the 2315 secret words.
* The 2315 secret words still need to be solved "in parallel" (submit a 2315 first guesses, receive feedback, submit 2315 second guesses, etc...)

This looked like it would solve one of the problems with the earlier contests: there was a non-trivial amount of luck involved in each submission. For a deterministic strategy some secret words are easier to determine than others, so your score depended on which words you received. In the first competition one of the other contestants ran a simulation to find the expected number of times they would need to submit to beat my submission. Despite having a worse average score, there was enough variance in their strategy that they could eventually succeed if they got lucky enough.

![](/images/wordle/hist.jpg)

*It was a pleasant surprise to see such a direct result of Central Limit Theorem emerge outside of a stats class.*

Testing every submission against the same set of words seems like it would mitigate the effect of luck, but there were still ways to introduce variance under these new rules. One common scenario near the "roots" of Wordle decision trees are when a pair of unknowns are narrowed down to only two possibilities, for example `fills`, and `pills`. An optimal deterministic strategy here would be to guess `fills` for both entries. The entry that was `fills` will be solved, and the next turn you can guess `pills` for the other, solving the two entries in 1.5 guesses on average. This also works out to 1.5 if you use `pills` for the first guess.

Alternatively, you could go with a non-deterministic strategy that guesses `fills` for the first entry, and `pills` for the second. If you're lucky, there's a 50% change you guessed correctly and solve both entries on the first guess. Otherwise, there's a 50% chance that you guessed both entries wrong, and solve both of them on the second turn. Note that this still has an expected 1.5 guesses on average, but the non-deterministic strategy has some variance that can be exploited by submitting over and over until you get lucky.

Well great, just add some variance to your strategy and spam submit until you have a solid lead! While possible, this felt unsatisfying -- there isn't much merit in being the contestant who submitted the most times. What would be satisfying would be to find a method that could outperform the best known deterministic strategy's score of 7920 *on average*, not just by getting lucky. By definition this couldn't be done with another deterministic strategy. But what if we took deterministic two strategies and "mixed" them? For example, use one strategy for the first 1000 entries, and use a second strategy for the rest?

In the first three contests, there was no advantage to using a mixed strategy -- if the best strategy took 3.421 guesses on average, mixing it with the second best strategy with 3.422 guesses on average would only make the expected value of your score worse. The fourth contest had a subtle, but important difference: secret words were sampled *without* replacement -- as soon as we identify a word we can remove it from the pool of possibilities for any unresolved entries. When using a single strategy this information is totally useless, since the turn we receive feedback that uniquely identifies a word is the same turn we receive feedback for other words that disambiguates them from the solved word anyway. With multiple strategies, there's a chance that one strategy can find information for another while its still actionable.

Consider the following scenario:

**TODO: generate a scenario lol**

Viz idea: TODO: cheat with prefixes using *ills

Possibilities | Guess 1 | Feedback 1 | Guess 2

WORD0            GUESS      GGYGY
WORD1            GUESS      GGYGY
WORD2            GUESS      GGYGY
WORD3            GUESS      GGGGY
WORD4            GUESS      GGGGY
WORD5            GUESS      GGYYY
WORD6            GUESS      GGYYY
WORD7            GUESS      GGYYY
WORD8            GUESS      GGYYY
WORD9            GUESS      GGYYY

^ add separator lines

The example given only considers mixing two strategies, however my best submission involved mixing the ten best strategies and sharing information between them, netting a score of 7574, a 4.4% improvement over the best performance for a single deterministic strategy. I found that mixing more than ten strategies started to yield diminishing returns. Possible explanations for this are:
* As we introduce more strategies, we need to involve "worse" starting words (strategies in the top 10 are mostly good, strategies in the top 50 not so much). This makes it harder for the new strategy to reveal enough useful information to other strategies to compensate for how bad it is in isolation
* The more strategies we introduce, the lower the chance that any single strategy has enough "samples" to reveal actionable information

Anyway, this was enough to secure the win for the fourth, and ultimately final Wordle competition, and lead to some interesting open questions:
* The information sharing strategy described only removes words from the pool of strategy if we're *certain* that the word is already somewhere else. In reality, you can determine the probability that a word is somewhere based on how many valid spots it can be in, and use this information when doing the next round's guesses. I suspect this might turn the game into an incredibly convoluted version of the [Monte Hall Problem](https://en.wikipedia.org/wiki/Monty_Hall_problem).
* What's the best mix of two starting words (nets lowest expected number of guesses)? I imagine this is harder than just mixing the two best strategies -- some strategy pairs might have better "synergy", i.e. reveal information that others might find "useful" more frequently. For what it's worth, I'm pretty sure computing this exactly is extremely intractable, but I'm frequently wrong about this sort of thing.
* What's the optimal mix of any number of starting words? (Probably super-duper intractable)
* My last submission only mixed strategies during the first turn. Can we do better by mixing strategies at other turns as well?
* Should I find less convoluted hobbies?

# Retrospective: was all of this effort worth it?

Probably not, although a handful of good things came out of it:
* I suspect that using Python has done irreparable damage to my ability to estimate just how fast things *can* be. The act of switching from Python to Rust without doing any application level optimization tricks sped up the time to build decision trees by an order of magnitude. It took me another hour before I recalled the words of my [foundation assigned Rust evangelist](https://twitter.com/exists_forall?lang=en): "Did you remember to use the `--release` flag?" This gave me another order of magnitude improvement for free, and was a nice wake-up call about just how slow Python is.
* It gave me a funny story and something worth doing a write-up on.
* I received some prize money for winning each of the competitions, about ~$80 of BTC in total. In itself this isn't worth a ton to me, but watching it decay in value by ~55% over the last few months was a valuable lesson in just how volatile cryptocurrency is, without having to stake any of my own money. A few weeks later while talking to a friend, they pointed out that the energy cost of a single Bitcoin transaction came out to about ~700 kW\*h. The fact that that much energy was burned *four times* to receive a relatively trivial amount of money was horrifying. A lesson learned, albeit the hard way.

On the downside, I now have an overwhelming amount of incredibly useless knowledge about optimal Wordle. While I'd heard the phrase "cursed for knowledge" before, I never truly understood it until Wordle came up while I was getting my teeth cleaned at the dentist:

**Oral Hygenist**: Have you heard of a game called Wordle? My whole family loves it!<br>
**Me**: Mhmha uhuh<br>
**Oral Hygenist**: My go to starting word is "ocean", but my daughter always starts with "raise"<br>
**Me**: *Mentally gauging how hard it would be to talk about optimal Wordle with a normal person*<br>
**Me**: Mmh yah thts ah gd wahn<br>

# Conclusion

Aiyah is a five-letter word.
