---
layout: post
img: https://i.imgur.com/dJbnYRo.png
title: Corollaries to Big Data™
---

Okay, well maybe not big data compared to what some companies churn through.
But definitely not small data for an individual, unless you're [one of *those*
people on reddit.](https://www.reddit.com/r/DataHoarder/) But, that's enough
with semantics.

To preface this entry, I will briefly try to butcher a description of one of the
research projects I'm involved with at my university's [neuroeconomics lab](neuroecon.berkeley.edu).
Explaining what exactly neuroeconomics is would probably take another blog post.
To put it succintly, neuroeconomics studies the intersection of behavioral
economics and neuroscience. To put it anecdotally, I'm a computer science major
in a neuroscience lab operating within the business school. 

¯\\ \_(ツ)\_/¯

Anyway, one of my most well-defined projects among other miscellaneous things is
to work with word embeddings: machine learning models of natural language that
"embed" words into a 300-dimensional vector space through completely
unsupervised training methods. The hope is to use these embeddings
to predict how people would behave in various decision-making problems. Getting
these models to be consistent however generally requires corpora of
100 million well-formed english sentences, at the very least. Enter our first 
corollary:

# Corollary 1: Small Storage

I don't know a lot about big data, but I can tell you all about low disk space.
Most of my first semester in the lab was plagued with running low on disk space
which turns out can be quite catastophic on a Linux system, where the philosophy
asserts that *everything* is a file. Needless to say, things can get a bit wonky
when the kernel becomes physically incapable of allocating space for new files.

Anyway, I took this as a sign/excuse to finally to get a fresh NVMe SSD
(in the words of a firmware friend, "there's no going back"), and so the problem
was temporarily alleviated. Temporarily. This semester we hired a few new
undergrads to help out, and as part of the oboarding process I was asked to send
out a dataset and a script to build a basic model. One person was able to get the dataset, one was able to download it but didn't have the space to decompress, and one couldn't even find space for the compressed archive. 

¯\\\_(ツ)\_/¯

# Corollary 2: Small Memory

All right. Well, clearly trying to distribute datasets like that to everyone
wasn't very efficient anyway. Ideally, we could just keep all our datasets on
a shared remote machine (preferably with looots of storage) and work from there.
Thankfully, the university's [Open Computing Facility](
https://www.ocf.berkeley.edu/) (whose members are, on an unrelated note, very
cool people) offers access to a high performance computing cluster which has
a roomy 7TB of available disk space. Ahhh. We can breathe again. Now we can
get back to work, right?

Nope. Well, I guess we could, but it wouldn't sit right. The HPC cluster is
technically just one machine, but is definitely the most impressive machine I've
ever gotten to work directly with. Namely, it has 2 CPU's running at 2.4 GHz,
with 20 threads each. In other words, "it'll train faster than my laptop."
Or so I thought. Actually, the gains were negligible, even after specifying to
train with 24+ worker threads. What's the matter?

After some digging, I found out that one of the major bottlenecks for word
embedding model training is the time it takes to feed the corpus to the
worker threads. After double checking the stats and making sure [Slurm](
https://slurm.schedmd.com/) (the workload manager used for the cluster) was 
allocating the correct amount of worker threads, it became evident that the
corpus iterator was really only quick enough to keep up with 4 threads of
execution while the other 20 remained idle. Okay, okay. We can just
pre-process the corpus and load the whole thing into memory, right?

Actually, yeah, that part actually worked. The reason I include this corollary
though is because there's no way it could have worked on anything other than a
high performance cluster. Loading the corpus (with a heapload of mysterious python overhead) into memory took up 50GB of RAM, which happens to work out 
since the machine had 252GB available. Truly a monster.

![More hands on deck just meant more idle hands](/images/cluster.png)

*Things of note: the silly amount of memory being used and the silly amount of 
memory available. Also worthy of note: the comparatively awkward amount of Swap.*

# Corollary 3: Slow Read Times

By the law of tripartite division, there is of a course a final corollary.
Despite my sheer determination to rule over every clock cycle, the bottleneck
still remains in memory. Which is to say, DRAM just isn't fast enough. Even with
the entire corpus pre-tokenized and loaded into memory, reads from memory just aren't quick enough to keep up with the number of available threads, and in fact
would perform only marginally better than if it had only 8 threads of execution,
which is exactly the capabilities of laptop (albeit at a lower clock rate).

Sure, training procedures have been sped up over tenfold. Sure,
we've come far. But alas, we can go no further. For this is a problem for the
*hardware people*.

¯\\\_(ツ)\_/¯